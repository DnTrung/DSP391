{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":434491,"sourceType":"datasetVersion","datasetId":196294}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/n090993/midi-dsp391?scriptVersionId=187344945\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"try:\n    import music21\nexcept:\n    !pip install music21\n    !sudo apt-get update\n    !sudo apt install musescore3 -y","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-23T12:49:28.873129Z","iopub.execute_input":"2024-06-23T12:49:28.873485Z","iopub.status.idle":"2024-06-23T12:49:29.224943Z","shell.execute_reply.started":"2024-06-23T12:49:28.873439Z","shell.execute_reply":"2024-06-23T12:49:29.224165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    import midi2audio\nexcept:\n    !pip install midi2audio\n    !sudo apt-get update\n    !sudo apt-get install fluidsynth -y","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-23T12:51:27.625844Z","iopub.execute_input":"2024-06-23T12:51:27.626701Z","iopub.status.idle":"2024-06-23T12:51:27.636885Z","shell.execute_reply.started":"2024-06-23T12:51:27.626661Z","shell.execute_reply":"2024-06-23T12:51:27.636083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install np_utils\n","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:51:31.351509Z","iopub.execute_input":"2024-06-23T12:51:31.351863Z","iopub.status.idle":"2024-06-23T12:51:46.264135Z","shell.execute_reply.started":"2024-06-23T12:51:31.351832Z","shell.execute_reply":"2024-06-23T12:51:46.263026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import IPython\nfrom IPython.display import Image, Audio\nfrom midi2audio import FluidSynth\nfrom music21 import corpus, converter, instrument, note, stream, chord, duration\nimport matplotlib.pyplot as plt\nimport time\nimport tensorflow as tf\n\nimport os\nimport pickle\nimport keras\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import plot_model\n\nimport os\nimport numpy as np\nimport glob\n\nfrom keras.layers import LSTM, Input, Dropout, Dense, Activation, Embedding, Concatenate, Reshape\nfrom keras.layers import Flatten, RepeatVector, Permute, TimeDistributed\nfrom keras.layers import Multiply, Lambda, Softmax\nimport keras.backend as K \nfrom keras.models import Model\nfrom tensorflow.keras.optimizers import RMSprop\nfrom keras.utils import to_categorical\n\n\nimport np_utils\nimport seaborn as sns\n","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:52:23.862077Z","iopub.execute_input":"2024-06-23T12:52:23.862965Z","iopub.status.idle":"2024-06-23T12:52:38.328621Z","shell.execute_reply.started":"2024-06-23T12:52:23.862928Z","shell.execute_reply":"2024-06-23T12:52:38.327832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nprint(\"Keras version:\", keras.__version__)\nprint(\"TensorFlow version:\", tf.__version__)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_name = '../input/classical-music-midi/chopin'\nfilename = 'chpn-p7'\nfile = \"{}/{}.mid\".format(dataset_name, filename)\n\noriginal_score = converter.parse(file).chordify()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:58:52.402323Z","iopub.execute_input":"2024-06-23T12:58:52.402695Z","iopub.status.idle":"2024-06-23T12:58:52.772948Z","shell.execute_reply.started":"2024-06-23T12:58:52.402665Z","shell.execute_reply":"2024-06-23T12:58:52.772048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fs = FluidSynth()\nfile = '..//input/classical-music-midi/chopin/chpn-p7.mid'\nfs.midi_to_audio(file, 'chpn-p7.wav')\nIPython.display.Audio(\"chpn-p7.wav\") ","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-06-23T12:58:55.376175Z","iopub.execute_input":"2024-06-23T12:58:55.376996Z","iopub.status.idle":"2024-06-23T12:59:01.839695Z","shell.execute_reply.started":"2024-06-23T12:58:55.376962Z","shell.execute_reply":"2024-06-23T12:59:01.838733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_score.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T07:38:49.991508Z","iopub.execute_input":"2024-06-23T07:38:49.991814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------------------\n# Extracting the data\n\nIt loops through the score and extracts the pitch and time of each note (and rest) into two lists. The entire chord is stored as a string, and individual notes in the chord are separated by dots. The male after the name of each note refers to the octave to which the note belongs.","metadata":{}},{"cell_type":"code","source":"notes = []\ndurations = []\n\nfor element in original_score.flat:    \n    if isinstance(element, chord.Chord):\n        notes.append('.'.join(n.nameWithOctave for n in element.pitches))\n        durations.append(element.duration.quarterLength)\n\n    if isinstance(element, note.Note):\n        if element.isRest:\n            notes.append(str(element.name))\n            durations.append(element.duration.quarterLength)\n        else:\n            notes.append(str(element.nameWithOctave))\n            durations.append(element.duration.quarterLength)   ","metadata":{"execution":{"iopub.status.busy":"2024-06-23T10:49:41.381045Z","iopub.execute_input":"2024-06-23T10:49:41.38191Z","iopub.status.idle":"2024-06-23T10:49:41.418227Z","shell.execute_reply.started":"2024-06-23T10:49:41.381872Z","shell.execute_reply":"2024-06-23T10:49:41.417332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\nduration', 'pitch')\nidx = 0\nfor n,d in zip(notes,durations):\n    if idx < 50:\n        print(d, '\\t', n)\n    idx = idx + 1   ","metadata":{"execution":{"iopub.status.busy":"2024-06-23T10:49:54.31739Z","iopub.execute_input":"2024-06-23T10:49:54.318016Z","iopub.status.idle":"2024-06-23T10:49:54.324228Z","shell.execute_reply.started":"2024-06-23T10:49:54.317981Z","shell.execute_reply":"2024-06-23T10:49:54.323345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(notes[:50])","metadata":{"execution":{"iopub.status.busy":"2024-06-23T10:50:02.192582Z","iopub.execute_input":"2024-06-23T10:50:02.193323Z","iopub.status.idle":"2024-06-23T10:50:02.198189Z","shell.execute_reply.started":"2024-06-23T10:50:02.19329Z","shell.execute_reply":"2024-06-23T10:50:02.19714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2024-06-23T10:50:21.623552Z","iopub.execute_input":"2024-06-23T10:50:21.624378Z","iopub.status.idle":"2024-06-23T10:50:21.630281Z","shell.execute_reply.started":"2024-06-23T10:50:21.624344Z","shell.execute_reply":"2024-06-23T10:50:21.629343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total notes in all the Chopin midis in the dataset:\", len(notes))","metadata":{"execution":{"iopub.status.busy":"2024-06-23T10:50:25.118201Z","iopub.execute_input":"2024-06-23T10:50:25.118542Z","iopub.status.idle":"2024-06-23T10:50:25.123859Z","shell.execute_reply.started":"2024-06-23T10:50:25.118515Z","shell.execute_reply":"2024-06-23T10:50:25.122804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_num = Counter(notes)\nprint(\"Total unique notes in the Corpus:\", len(count_num))","metadata":{"execution":{"iopub.status.busy":"2024-06-23T10:50:49.701599Z","iopub.execute_input":"2024-06-23T10:50:49.702423Z","iopub.status.idle":"2024-06-23T10:50:49.707015Z","shell.execute_reply.started":"2024-06-23T10:50:49.702388Z","shell.execute_reply":"2024-06-23T10:50:49.706115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Notes = list(count_num.keys())\nRecurrence = list(count_num.values())\n#Average recurrenc for a note in Corpus\ndef Average(lst):\n    return sum(lst) / len(lst)\nprint(\"Average recurrenc for a note in Corpus:\", Average(Recurrence))\nprint(\"Most frequent note in Corpus appeared:\", max(Recurrence), \"times\")\nprint(\"Least frequent note in Corpus appeared:\", min(Recurrence), \"time\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T10:50:54.087275Z","iopub.execute_input":"2024-06-23T10:50:54.088057Z","iopub.status.idle":"2024-06-23T10:50:54.094125Z","shell.execute_reply.started":"2024-06-23T10:50:54.088023Z","shell.execute_reply":"2024-06-23T10:50:54.093132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defiing Helper Functions","metadata":{}},{"cell_type":"code","source":"def get_music_list(data_folder):    \n    file_list = glob.glob(os.path.join(data_folder, \"*.mid\"))\n    parser = converter    \n    return file_list, parser\n\ndef create_network(n_notes, n_durations, embed_size=100, rnn_units=256, use_attention=False):\n    notes_in = Input(shape=(None,))\n    durations_in = Input(shape=(None,))\n\n    x1 = Embedding(n_notes, embed_size)(notes_in)\n    x2 = Embedding(n_durations, embed_size)(durations_in)\n    x = Concatenate()([x1, x2])\n    x = LSTM(rnn_units, return_sequences=True)(x)\n\n    if use_attention:\n        x = LSTM(rnn_units, return_sequences=True)(x)\n        e = Dense(1, activation='tanh')(x)\n        e = Reshape([-1])(e)\n        alpha = Activation('softmax')(e)\n        alpha_repeated = Permute([2, 1])(RepeatVector(rnn_units)(alpha))\n        c = Multiply()([x, alpha_repeated])\n        c = Lambda(lambda xin: tf.keras.backend.sum(xin, axis=1), output_shape=(rnn_units,))(c)\n    else:\n        c = LSTM(rnn_units)(x)\n\n    notes_out = Dense(n_notes, activation='softmax', name='pitch')(c)\n    durations_out = Dense(n_durations, activation='softmax', name='duration')(c)\n\n    model = Model([notes_in, durations_in], [notes_out, durations_out])\n\n    if use_attention:\n        att_model = Model([notes_in, durations_in], alpha)\n    else:\n        att_model = None\n\n    opti = RMSprop(learning_rate=0.001)\n    model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy'], optimizer=opti, metrics=['accuracy','accuracy']\n )\n\n    return model, att_model\n\n\ndef get_distinct(elements):\n    # Get all pitch names\n    element_names = sorted(set(elements))\n    n_elements = len(element_names)\n    return (element_names, n_elements)\n\ndef create_lookups(element_names):\n    # create dictionary to map notes and durations to integers\n    element_to_int = dict((element, number) for number, element in enumerate(element_names))\n    int_to_element = dict((number, element) for number, element in enumerate(element_names))\n    return (element_to_int, int_to_element)    \n\ndef prepare_sequences(notes, durations, lookups, distincts, seq_len =32):\n    note_to_int, int_to_note, duration_to_int, int_to_duration = lookups\n    note_names, n_notes, duration_names, n_durations = distincts\n\n    notes_network_input = []\n    notes_network_output = []\n    durations_network_input = []\n    durations_network_output = []\n\n    # create input sequences and the corresponding outputs\n    for i in range(len(notes) - seq_len):\n        notes_sequence_in = notes[i:i + seq_len]\n        notes_sequence_out = notes[i + seq_len]\n        notes_network_input.append([note_to_int[char] for char in notes_sequence_in])\n        notes_network_output.append(note_to_int[notes_sequence_out])\n\n        durations_sequence_in = durations[i:i + seq_len]\n        durations_sequence_out = durations[i + seq_len]\n        durations_network_input.append([duration_to_int[char] for char in durations_sequence_in])\n        durations_network_output.append(duration_to_int[durations_sequence_out])\n\n    n_patterns = len(notes_network_input)\n\n    # reshape the input into a format compatible with LSTM layers\n    notes_network_input = np.reshape(notes_network_input, (n_patterns, seq_len))\n    durations_network_input = np.reshape(durations_network_input, (n_patterns, seq_len))\n    network_input = [notes_network_input, durations_network_input]\n\n    notes_network_output = to_categorical(notes_network_output, num_classes=n_notes)\n    durations_network_output = to_categorical(durations_network_output, num_classes=n_durations)\n    network_output = [notes_network_output, durations_network_output]\n    return (network_input, network_output)\n\ndef sample_with_temp(preds, temperature):\n    if temperature == 0:\n        return np.argmax(preds)\n    else:\n        preds = np.log(preds) / temperature\n        exp_preds = np.exp(preds)\n        preds = exp_preds / np.sum(exp_preds)\n        return np.random.choice(len(preds), p=preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run params\nrun_folder = '/kaggle/working'\n\nstore_folder = os.path.join(run_folder, 'store')\ndata_folder ='..//input/classical-music-midi/chopin'\n\nif not os.path.exists('store'):\n    os.mkdir(os.path.join(run_folder, 'store'))\n    os.mkdir(os.path.join(run_folder, 'output'))\n    os.mkdir(os.path.join(run_folder, 'weights'))\n    os.mkdir(os.path.join(run_folder, 'viz'))\n\nmode = 'build'\n\n# data params\nintervals = range(1)\nseq_len = 64\n\n# model params\nembed_size = 100\nrnn_units = 256\nuse_attention = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if mode == 'build':    \n    music_list, parser = get_music_list(data_folder)\n    print(len(music_list), 'files in total')\n\n    notes = []\n    durations = []\n\n    for i, file in enumerate(music_list):\n        print(i+1, \"Parsing %s\" % file)\n        print(file)\n        original_score = parser.parse(file).chordify()        \n        for interval in intervals:\n            score = original_score.transpose(interval)\n\n            notes.extend(['START'] * seq_len)\n            durations.extend([0]* seq_len)\n\n            for element in score.flat:                \n                if isinstance(element, note.Note):\n                    if element.isRest:\n                        notes.append(str(element.name))\n                        durations.append(element.duration.quarterLength)\n                    else:\n                        notes.append(str(element.nameWithOctave))\n                        durations.append(element.duration.quarterLength)\n                        \n                if isinstance(element, chord.Chord):\n                    notes.append('.'.join(n.nameWithOctave for n in element.pitches))\n                    durations.append(element.duration.quarterLength)\n\n    with open(os.path.join(store_folder, 'notes'), 'wb') as f:\n        pickle.dump(notes, f) \n    with open(os.path.join(store_folder, 'durations'), 'wb') as f:\n        pickle.dump(durations, f) \nelse:\n    with open(os.path.join(store_folder, 'notes'), 'rb') as f:\n        notes = pickle.load(f)\n    with open(os.path.join(store_folder, 'durations'), 'rb') as f:\n        durations = pickle.load(f) ","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------\n# Embedding Note and Duration\n\nTo create a dataset for training the model, we first convert the pitch and tempo into integer values. It doesn't matter what these values are because we use an embedding layer to convert the integer to a vector.","metadata":{}},{"cell_type":"code","source":"# get the distinct sets of notes and durations\nnote_names, n_notes = get_distinct(notes)\nduration_names, n_durations = get_distinct(durations)\ndistincts = [note_names, n_notes, duration_names, n_durations]\n\nwith open(os.path.join(store_folder, 'distincts'), 'wb') as f:\n    pickle.dump(distincts, f)\n\n# make the lookup dictionaries for notes and dictionaries and save\nnote_to_int, int_to_note = create_lookups(note_names)\nduration_to_int, int_to_duration = create_lookups(duration_names)\nlookups = [note_to_int, int_to_note, duration_to_int, int_to_duration]\n\nwith open(os.path.join(store_folder, 'lookups'), 'wb') as f:\n    pickle.dump(lookups, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\nnote_to_int')\nfor i, item in enumerate(note_to_int.items()):\n    if i < 10:\n        print(item)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\nduration_to_int')\nduration_to_int","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"network_input, network_output = prepare_sequences(notes, durations, lookups, distincts, seq_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Divide the dataset by 32 notes to create the training set. Target is the next pitch and time signature in the sequence.","metadata":{}},{"cell_type":"code","source":"print('pitch input')\nprint(network_input[0][0])\nprint('duration input')\nprint(network_input[1][0])\nprint('pitch target')\nprint(network_output[0][0])\nprint('duration target')\nprint(network_output[1][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------\n# Modeling","metadata":{}},{"cell_type":"code","source":"model, att_model = create_network(n_notes, n_durations, embed_size, rnn_units, use_attention)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, to_file=os.path.join(run_folder ,'viz/model.png'), show_shapes = True, show_layer_names = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------------------------------------------\n# Training","metadata":{}},{"cell_type":"code","source":"import os\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n\n\nweights_folder = os.path.join(run_folder, 'weights')\n\ncheckpoint1 = ModelCheckpoint(\n    os.path.join(weights_folder, \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.keras\"),\n    monitor='loss',\n    verbose=0,\n    save_best_only=True,\n    mode='min'\n)\n\ncheckpoint2 = ModelCheckpoint(\n    os.path.join(weights_folder, \"weights.keras\"),\n    monitor='loss',\n    verbose=0,\n    save_best_only=True,\n    mode='min'\n)\n\nearly_stopping = EarlyStopping(\n    monitor='loss',\n    restore_best_weights=True,\n    patience=10\n)\n\n\ncallbacks_list = [\n    checkpoint1,\n    checkpoint2,\n    early_stopping]\n\n# Ensure the weights file has the correct extension for saving weights\nweights_file = os.path.join(weights_folder, \"weights.weights.h5\")\nmodel.save_weights(weights_file)\nmodel_json = model.to_json()\nwith open(\"/kaggle/working//model_architecture.json\", \"w\") as json_file:\n    json_file.write(model_json)\n\nhistory = model.fit(\n    network_input, network_output,\n    epochs=150, batch_size=32,\n    validation_split=0.2,\n    callbacks=callbacks_list,\n    shuffle=True\n)\n\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(history.history.keys())\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------------\n# Predicting","metadata":{}},{"cell_type":"code","source":"# prediction params\nnotes_temp=0.5\nduration_temp = 0.5\nmax_extra_notes = 100\nmax_seq_len = 64\nseq_len = 64\n\nnotes = ['START']\ndurations = [0]\n\nif seq_len is not None:\n    notes = ['START'] * (seq_len - len(notes)) + notes\n    durations = [0] * (seq_len - len(durations)) + durations\n\nsequence_length = len(notes)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T09:46:25.082883Z","iopub.execute_input":"2024-07-08T09:46:25.083154Z","iopub.status.idle":"2024-07-08T09:46:25.095638Z","shell.execute_reply.started":"2024-07-08T09:46:25.083132Z","shell.execute_reply":"2024-07-08T09:46:25.094704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_output = []\nnotes_input_sequence = []\ndurations_input_sequence = []\noverall_preds = []\n\nfor n, d in zip(notes,durations):\n    note_int = note_to_int[n]\n    duration_int = duration_to_int[d]\n    \n    notes_input_sequence.append(note_int)\n    durations_input_sequence.append(duration_int)\n    \n    prediction_output.append([n, d])\n    \n    if n != 'START':\n        midi_note = note.Note(n)\n        new_note = np.zeros(256)\n        new_note[midi_note.pitch.midi] = 1\n        overall_preds.append(new_note)\n\natt_matrix = np.zeros(shape = (max_extra_notes+sequence_length, max_extra_notes))\n\nfor note_index in range(max_extra_notes):\n\n    prediction_input = [\n        np.array([notes_input_sequence])\n        , np.array([durations_input_sequence])\n       ]\n\n    notes_prediction, durations_prediction = model.predict(prediction_input, verbose=0)\n    if use_attention:\n        att_prediction = att_model.predict(prediction_input, verbose=0)[0]\n        att_matrix[(note_index-len(att_prediction)+sequence_length):(note_index+sequence_length), note_index] = att_prediction\n    \n    new_note = np.zeros(256)\n    \n    for idx, n_i in enumerate(notes_prediction[0]):\n        try:\n            note_name = int_to_note[idx]\n            midi_note = note.Note(note_name)\n            new_note[midi_note.pitch.midi] = n_i            \n        except:\n            pass\n        \n    overall_preds.append(new_note)            \n    \n    i1 = sample_with_temp(notes_prediction[0], notes_temp)\n    i2 = sample_with_temp(durations_prediction[0], duration_temp)    \n\n    note_result = int_to_note[i1]\n    duration_result = int_to_duration[i2]\n    \n    prediction_output.append([note_result, duration_result])\n\n    notes_input_sequence.append(i1)\n    durations_input_sequence.append(i2)\n    \n    if len(notes_input_sequence) > max_seq_len:\n        notes_input_sequence = notes_input_sequence[1:]\n        durations_input_sequence = durations_input_sequence[1:]\n        \n    if note_result == 'START':\n        break\n\noverall_preds = np.transpose(np.array(overall_preds)) \nprint('Generated sequence of {} notes'.format(len(prediction_output)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------------------------------------\n# Intrepreting Model","metadata":{}},{"cell_type":"markdown","source":"-------------------------------------------\n## Heatmap","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,15))\nax.set_yticks([int(j) for j in range(35,70)])\nplt.imshow(overall_preds[35:70,:], origin=\"lower\", cmap='coolwarm', vmin = -0.5, vmax = 0.5, extent=[0, max_extra_notes, 35,70])\nplt.xlabel(\"Note number\",fontsize=20)\nplt.ylabel(\"Pitch value (MIDI number)\",fontsize=20)\nplt.title(\"Probability distribution of the next possible note over time\",fontsize=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_folder = os.path.join(run_folder, 'output')\n\nmidi_stream = stream.Stream()\n\n# create note and chord objects based on the values generated by the model\nfor pattern in prediction_output:\n    note_pattern, duration_pattern = pattern\n    # pattern is a chord\n    if ('.' in note_pattern):\n        notes_in_chord = note_pattern.split('.')\n        chord_notes = []\n        for current_note in notes_in_chord:\n            new_note = note.Note(current_note)\n            new_note.duration = duration.Duration(duration_pattern)\n            new_note.storedInstrument = instrument.Violoncello()\n            chord_notes.append(new_note)\n        new_chord = chord.Chord(chord_notes)\n        midi_stream.append(new_chord)\n    elif note_pattern == 'rest':\n    # pattern is a rest\n        new_note = note.Rest()\n        new_note.duration = duration.Duration(duration_pattern)\n        new_note.storedInstrument = instrument.Violoncello()\n        midi_stream.append(new_note)\n    elif note_pattern != 'START':\n    # pattern is a note\n        new_note = note.Note(note_pattern)\n        new_note.duration = duration.Duration(duration_pattern)\n        new_note.storedInstrument = instrument.Violoncello()\n        midi_stream.append(new_note)\n\nmidi_stream = midi_stream.chordify()\ntimestr = time.strftime(\"%Y%m%d-%H%M%S\")\nnew_file = 'output-' + timestr + '.mid'\nmidi_stream.write('midi', fp=os.path.join(output_folder, new_file))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_path = '/kaggle/working/output/'+new_file\nfs = FluidSynth()\nfs.midi_to_audio(new_path, 'new_output3.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_attention:\n    fig, ax = plt.subplots(figsize=(20,20))\n    im = ax.imshow(att_matrix[(seq_len-2):,], cmap='coolwarm', interpolation='nearest')    \n\n    # Minor ticks\n    ax.set_xticks(np.arange(-.5, len(prediction_output)- seq_len, 1), minor=True);\n    ax.set_yticks(np.arange(-.5, len(prediction_output)- seq_len, 1), minor=True);\n\n    # Gridlines based on minor ticks\n    ax.grid(which='minor', color='black', linestyle='-', linewidth=1)    \n    \n    # We want to show all ticks...\n    ax.set_xticks(np.arange(len(prediction_output) - seq_len))\n    ax.set_yticks(np.arange(len(prediction_output)- seq_len+2))\n    # ... and label them with the respective list entries\n    ax.set_xticklabels([n[0] for n in prediction_output[(seq_len):]])\n    ax.set_yticklabels([n[0] for n in prediction_output[(seq_len - 2):]])\n    ax.xaxis.tick_top()    \n    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"left\", va = \"center\", rotation_mode=\"anchor\")\n    plt.xlabel(\"sequence of generated notes\",fontsize=20)\n    plt.ylabel(\"The point of attention\",fontsize=20)\n    plt.title(\"The amount of attention given to the network hidden state\",fontsize=30)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----------------------------------\n# Let's compare the original performance with the new one.","metadata":{}},{"cell_type":"markdown","source":"---------------------------------------\n## Let's listen originals","metadata":{}},{"cell_type":"code","source":"IPython.display.Audio(\"chpn-p7.wav\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_score = converter.parse(new_path).chordify()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------------------------------\n## Newly composed music\n\nFinally, let's listen to the music we made with our model.","metadata":{}},{"cell_type":"code","source":"IPython.display.Audio(\"new_output3.wav\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_score.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr style=\"border: solid 3px blue;\">","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}